# DeepSeek的进化路线

DeepSeek相对GPT的创新点：强化+蒸馏+便宜

|                   | Deepseek-v1                                                  | Deepseek-v2                                                  | Deepseek-v3                                                  | Deepseek-V1                                                  |
| ----------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 发布日期          | 2024-01                                                      | 2024-05                                                      | 2024-12                                                      | 2025-01-20                                                   |
| 论文              | [v1论文](https://arxiv.org/abs/2401.06066)                   | [v2论文](https://arxiv.org/abs/2405.04434)                   | [v3论文](https://arxiv.org/html/2412.19437)                  | [r1论文](https://arxiv.org/abs/2501.12948)                   |
| 代码              | [v1代码](https://github.com/deepseek-ai/deepseek-LLM)        | [v2代码](https://github.com/deepseek-ai/DeepSeek-V2)         | [v3代码](https://github.com/deepseek-ai/DeepSeek-V3)         | [r1代码](https://github.com/deepseek-ai/DeepSeek-R1)         |
| 模型下载          | [v1下载](https://huggingface.co/deepseek-ai/deepseek-moe-16b-chat) | [v2下载](https://huggingface.co/deepseek-ai/DeepSeek-V2)     | [v3下载](https://huggingface.co/deepseek-ai/DeepSeek-V3)     | [r1下载](https://huggingface.co/deepseek-ai/DeepSeek-R1)     |
| Total Params      | 1.89B                                                        | 236B                                                         | 671B                                                         | 610B                                                         |
| Activated Params  | 0.24B                                                        | 21B                                                          | 37B                                                          | 37B                                                          |
| Context Length    | 128K                                                         | 128K                                                         | 128K                                                         | 128K                                                         |
| Nums of Experts   | 1+7                                                          | 2+6                                                          | 1+8                                                          | -                                                            |
| Activated Experts | 1+63                                                         | 2+160                                                        | 1+256                                                        | -                                                            |
| 核心优势          | 上下文编码                                                   | 开源低成本                                                   | 多领域性能领先、生成速度快                                   | 多模态与复杂推论                                             |
| 应用场景          | 文档处理、代码生成                                           | 科研、商业开发                                               | 长文本处理、代码、数学任务                                   | 跨模态交互、决策优化                                         |
| Idea              | 模型的主体结构基本沿用 LLaMA 的体系结构, 在注意力机制方面, 7B 模型使用 多头注意力 Multi-Head attention  (MHA),而 67B 模型使用 Grouped-Query Attention (GQA)替代 MHA 用来降低成本 | 多头潜在注意力机制(Multi-head Latent Attention,MLA) 和 DeepSeekMoE 架构 | 1.对 DeepSeekMoE 中的多专家负载均衡问题，提出了无辅助损失负载均衡策略(auxiliary-loss-free strategy ) ，相比使用辅助 loss 提升了模型性能<br>2.引入多 Token 预测(Multi-Token Prediction,MTP)技术，相比原来每次只能预测一个 token，显著提升了 infer 的速度。 | 1.采用 GROP(Group Relative Policy Optimization)算法<br>2.Reward Modeling :一种基于规则的奖励系统和语言一致性奖励系统<br>3.Cold Start:使用数千条冷启动数据 |

